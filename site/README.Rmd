---
output: 
  html_document:
    keep_md: yes
    css: custom.css
    toc: yes
  
---

## Semi-parametric Area Under the Curve (sAUC) Regression
Perform AUC analyses with discrete covariates and a semi-parametric estimation

### What is sAUC model and why?

In many applications, comparing two groups while adjusting for multiple covariates is desired for the statistical analysis.  For instance, in clinical trials, adjusting for covariates is a necessary aspect of the statistical analysis in order to improve the precision of the treatment comparison and to assess effect modification. sAUC is a semi-parametric AUC regression model to compare the effect of two treatment groups in the intended non-normal outcome while adjusting for discrete covariates. More detailed reasons on what it is and why it is proposed are outlined in [this paper](https://sbohora.github.io/sAUC/articles/bohora-etal-sauc-paper.pdf). A major reason behind the development of this method is that this method is computationally simple and is based on closed-form parameter and standard error estimation.

### Model

We consider applications that compare a response variable y between two groups (A and B) while adjusting for k categorical covariates $X_1,X_2,...,X_k$.  The response variable y is a continuous or ordinal variable that is not normally distributed.  Without loss of generality, we assume each covariate is coded such that $X_i=1,...,n_i$,for $i=1,...,k$. For each combination of the levels of the covariates, we define the Area Under the ROC curve (AUC) in the following way:

$$\pi_{x_1 x_2...x_k}=P(Y^A>Y^B|X_1=x_1,X_2=x_2,...,X_k=x_k ) + $$ 

$$\frac{1}{2} P(Y^A=Y^B|X_1=x_1,X_2=x_2,...,X_k=x_k ),$$

where $x_1=1,...,n_1,...,x_k=1,...,n_k$, and $Y^A$ and $Y^B$ are two randomly chosen observations from Group A and B, respectively.  The second term in the above equation is for the purpose of accounting ties.

For each covariate $X_i$, without loss of generality, we use the last category as the reference category and define ($n_i-1$) dummy variables $X_i^{(1)},X_i^{(2)},...,X_i^{(n_i-1)}$ such that 

$$X_i^{(j)} (x)= \left\{\begin{array}
{rrr}
1, j = x \\
0, j \ne x,
\end{array}\right.
$$

where $i=1,...,k; j=1,...,n_i-1; x=1,...,n_i$.   We model the association between AUC $\pi_{x_1 x_2...x_k}$ and covariates using a logistic model.  Such a model specifies that the logit of $\pi_{x_1 x_2...x_k}$ is a linear combination of terms that are products of the dummy variables defined above.  Specifically,

$$logit(\pi_{x_1 x_2...x_k } )=Z_{x_1 x_2...x_k} \boldsymbol{\beta},$$ 

where $Z_{x_1 x_2...x_k}$ is a row vector whose elements are zeroes or ones and are products of $X_1^{(1)} (x_1 ),...,X_1^{(n_i-1) } (x_1),...,X_k^{(1)} (x_k),...,X_k^{(n_k-1)} (x_k)$, and $\boldsymbol{\beta}$ is a column vector of nonrandom unknown parameters.  Now, define a column vector $\pi$ by stacking up $\pi_{x_1 x_2...x_k}$ and define a matrix Z by stacking up $Z_{x_1 x_2...x_k}$, as $x_i$ ranges from 1 to $n_i, i=1,...,k$, our final model is  

$$logit(\pi)=Z\boldsymbol{\beta} ...(1)$$

The reason for us to use a logit transformation of the AUC instead of using the original AUC is for variance stabilization.  We will illustrate the above general model using examples.


### Estimation

First, we denote the number of observations with covariates $X_1=i_1,...,X_k=i_k$ in groups A and B by $N_{i_1...i_k}^A$ and $N_{i_1...i_k}^B$, respectively.  We assume both $N_{i_1...i_k}^A$ and $N_{i_1...i_k}^B$ are greater than zero in the following development.  An unbiased estimator of $\pi_{i_1...i_k}$ proposed by Mann and Whitney (1947) is

$$\hat{\pi}_{i_1...i_k}=\frac{\sum_{l=1}^{N_{i_1...i_k}^A} \sum_{j=1}^{N_{i_1...i_k}^B} I_{lj}}{N_{i_1...i_k}^A N_{i_1...i_k}^B},$$

where 

$$I_{i_1... i_k; lj}= \left\{\begin{array}
{rrr}
1, Y_{i_1...i_k; l}^A>Y_{i_1...i_k; j}^B \\
\frac{1}{2}, Y_{i_1...i_k; l}^A=Y_{i_1...i_k; j}^B \\
0, Y_{i_1...i_k; l}^A<Y_{i_1...i_k; j}^B
\end{array}\right.
$$

and $Y_{i_1...i_k; l}^A$ and $Y_{i_1...i_k; j}^B$ are observations with $X_1=i_1,...,X_k=i_k$ in groups A and B, respectively.  Delong, Delong and Clarke-Pearson (1988) have shown that 

$$\hat{\pi}_{i_1...i_k} \approx N(\pi_{i_1...i_k},\sigma_{i_1...i_k}^2).$$	

In order to obtain an estimator for $\sigma_{i_1...i_k}^2$, they first computed

$$V_{i_1...i_k; l}^A=\frac{1}{N_{i_1...i_k}^B } \sum_{j=1}^{N_{i_1...i_k}^B} I_{lj},  	l=1,...,N_{i_1...i_k}^A$$

and

$$V_{i_1...i_k;j}^B=\frac{1}{N_{i_1...i_k}^A } \sum_{l=1}^{N_{i_1...i_k}^A} I_{lj},  	j=1,...,N_{i_1...i_k}^B$$

Then, an estimate of the variance of the nonparametric AUC was

$$\hat{\sigma}_{i_1...i_k}^2=\frac{(s_{i_1...i_k}^A )^2}{N_{i_1...i_k}^A} + \frac{(s_{i_1...i_k}^B )^2}{N_{i_1...i_k}^B},$$

where 

$(s_{i_1...i_k}^A )^2$ and $(s_{i_1...i_k}^B )^2$ were the sample variances of 

$V_{i_1...i_k; l}^A; l=1,...,N_{i_1...i_k}^A$ and $V_{i_1...i_k; j}^B; j=1,...,N_{i_1...i_k}^B,$ respectively.  Clearly, we need both $N_{i_1...i_k}^A$ and $N_{i_1...i_k}^B$ are greater than two in order to compute $\hat{\sigma}_{i_1...i_k}^2$.

Now, in order to estimate parameters in Model (1), we first derive the asymptotic variance of $\hat{\gamma}_{i_1...i_k}$ using the delta method, which results in

$$\hat{\gamma}_{i_1...i_k}=logit(\hat{\pi}_{i_1...i_k}) \approx N(logit(\pi_{i_1...i_k}),\tau_{i_1...i_k}^2),$$

where $$\hat{\tau}_{i_1...i_k}^2=\frac{\hat{\gamma}_{i_1...i_k}^2}{\hat{\pi}_{i_1...i_k}^2  (1-\hat{\pi}_{i_1...i_k})^2}$$ 

Rewriting the above model, we obtain

$$\hat{\gamma}_{i_1...i_k}=logit(\pi_{i_1...i_k }) =Z_{i_1...i_k} \boldsymbol{\beta} + \epsilon_{i_1...i_k}$$
         
where, 

$\epsilon_{i_1,...,i_k} \approx N(0,\tau_{i_1,...,i_k}^2)$.  Then, by stacking up the $\hat{\gamma}_{1_i,...,i_k}$ to be 
$\hat{\gamma}, Z_{i_1...i_k}$ to be $\boldsymbol{Z}$, and $\epsilon_{i_1,...,i_k}$ to be 
$\boldsymbol{\epsilon}$, we have

$$\boldsymbol{\hat{\gamma}} =logit \boldsymbol{\hat{\pi}} = \boldsymbol{Z\beta + \epsilon},$$ 

where, $E(\epsilon)=0$ and $\hat{T}=Var(\epsilon)=diag(\hat{\tau}_{i_1... i_k}^2)$ which is a diagonal matrix.  Finally, by using the generalized least squares method, we estimate the parameters β  and its variance-covariance matrix as follows;

$$\boldsymbol{\hat{\beta} ={(\hat{Z}^T  \hat{T}^{-1}  Z)}^{-1} Z^T  \hat{T}^{-1} \hat{\gamma}}$$		

and
$$\hat{V}(\boldsymbol{\hat{\beta}}) = \boldsymbol{{(\hat{Z}^T  \hat{T}^{-1}  Z)}^{-1}}$$

The above equations can be used to construct a 100(1-α)% Wald confidence intervals for $\boldsymbol{\beta_i}$ using formula

$$\hat{\beta}_i \pm Z_{1-\frac{\alpha}{2}} \sqrt{\hat{V}(\hat{\beta}_i)},$$

where $Z_{1-\frac{\alpha}{2}}$ is the $(1-\frac{\alpha}{2})^{th}$ quantile of the standard normal distribution.  Equivalently, we reject 

$H_0:\beta_i = 0$  if $|\hat{\beta}_i| > Z_{1-\frac{\alpha}{2}} \sqrt{\hat{V}(\hat{\beta}_i)},$

The p-value for testing $H_0$ is $2 * P(Z > |\hat{\beta}_i|/\sqrt{\hat{V}\hat{\beta}_i}),$


where Z is a random variable with the standard normal distribution.

Now, the total number of cells (combinations of covariates $X_1,...,X_k$ is $n_1 n_2...n_k$. As mentioned earlier, for a cell to be usable in the estimation, the cell needs to have at least two observations from Group A and two observations from Group B.  As long as the total number of usable cells is larger than the dimension of $\boldsymbol{\beta}$, then the matrix ${\boldsymbol{\hat{Z}^T  \hat{T}^{-1}  Z}}$ is invertible and consequently,$\boldsymbol{\hat{\beta}}$ is computable and model (1) is identifiable.

